{"metadata":{"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8103487,"sourceType":"datasetVersion","datasetId":4785800},{"sourceId":8108338,"sourceType":"datasetVersion","datasetId":4789427},{"sourceId":8105322,"sourceType":"datasetVersion","datasetId":4787104}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import csv\nimport os\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\nfrom tensorflow.keras.models import load_model","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:13:15.858898Z","iopub.execute_input":"2024-04-13T13:13:15.859554Z","iopub.status.idle":"2024-04-13T13:13:15.864203Z","shell.execute_reply.started":"2024-04-13T13:13:15.859522Z","shell.execute_reply":"2024-04-13T13:13:15.863015Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# code to stitch up any video from data\ndef stitch_frames(sample_folder):\n    frames = []\n    for img_path in os.listdir( sample_folder):\n        frame_path = os.path.join(sample_folder, img_path)\n        frame = cv2.imread(frame_path)\n        frames.append(frame)\n    return frames\n\ndata_dir=\"/kaggle/input/ps2-testset/PS2 Test\"\n\nvideo_folder = 'video61'\noutput_video_path = \"output2.mp4\"\n\n# Sort sample folders\nsample_folders = sorted(os.listdir(os.path.join(data_dir,video_folder)))\nimg=os.listdir(os.path.join(data_dir,video_folder, sample_folders[0]))\n# Initialize video writer\nframe_height, frame_width, _ = cv2.imread(os.path.join(data_dir,video_folder, sample_folders[0], img[0])).shape\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\n\nout = cv2.VideoWriter(output_video_path, fourcc, 10, (frame_width, frame_height))\n\n# Stitch frames and write to video\nfor sample in sample_folders:\n    sample_folder = os.path.join(data_dir,video_folder, sample)\n    \n    frames = stitch_frames(sample_folder)\n    for frame in frames:\n        out.write(frame)\n\n# Release video writer\nout.release()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:13:44.278651Z","iopub.execute_input":"2024-04-13T13:13:44.279035Z","iopub.status.idle":"2024-04-13T13:13:49.284980Z","shell.execute_reply.started":"2024-04-13T13:13:44.279007Z","shell.execute_reply":"2024-04-13T13:13:49.283976Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# preprocessing for the input images\ndef detect_black_border(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)[1]\n    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    largest_contour = sorted(contours, key=cv2.contourArea, reverse=True)[0]\n    x, y, w, h = cv2.boundingRect(largest_contour)\n    shrink_factor = 0.05\n    x += int(w * shrink_factor)\n    y += int(h * shrink_factor)\n    w = int(w * (1 - 2 * shrink_factor))\n    h = int(h * (1 - 2 * shrink_factor))\n    return x, y, w, h\n\ndef img_preprocess(img, x, y, w, h, target_size=(224, 224)):\n    image = img\n    if x >= 0 and y >= 0 and w > 0 and h > 0:\n        bbox = (x, y, x + w, y + h)\n        cropped_image = Image.fromarray(image).crop(bbox)\n        resized_image = cropped_image.resize(target_size, Image.BILINEAR)\n    else:\n        resized_image = cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)\n    return img_to_array(resized_image)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:41:03.207390Z","iopub.execute_input":"2024-04-13T12:41:03.207896Z","iopub.status.idle":"2024-04-13T12:41:03.218925Z","shell.execute_reply.started":"2024-04-13T12:41:03.207864Z","shell.execute_reply":"2024-04-13T12:41:03.217741Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"\n\n# change this variable to the path of the saved model uploaded in the models folder as pixelcnn.h5\nmodel=load_model(\"/kaggle/input/final-model/pixelcnn.h5\")","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:41:06.240386Z","iopub.execute_input":"2024-04-13T12:41:06.241596Z","iopub.status.idle":"2024-04-13T12:41:07.651932Z","shell.execute_reply.started":"2024-04-13T12:41:06.241554Z","shell.execute_reply":"2024-04-13T12:41:07.650958Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#code for detecting smoke from realtime mp4 videos. frames are taken 10 at a time\n# detect smoke function to detect smoke using HSV thresholding techniques\ndef get_hsv(image):\n    blur = cv2.GaussianBlur(image, (21, 21), 0)\n    hsv = cv2.cvtColor(blur, cv2.COLOR_BGR2HSV)\n    lower = np.array([0, 0, 50], dtype=\"uint8\")\n    upper = np.array([179, 50, 255], dtype=\"uint8\")\n    mask = cv2.inRange(hsv, lower, upper)\n    no_smoke_pixels = cv2.countNonZero(mask)\n\n    # Threshold of 12600 pixels has been set\n    return no_smoke_pixels > 12600\n\ndef predict_smoke(frames):\n    res=[]\n    \n    for frame in frames:\n        res.append(get_hsv(frame))\n\n            \n    return 1 if round(res.count(True)/10)==0 else 0\n    \n\n\ndef label_frame(frame, label):\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    bottomLeftCornerOfText = (int(frame.shape[1]/2 - 100), frame.shape[0] - 20)\n    fontScale = 1\n    fontColor = (0, 0, 255) if label == 1 else (0, 255, 0)\n    lineType = 2\n\n    cv2.putText(frame, label, \n                bottomLeftCornerOfText, \n                font, \n                fontScale,\n                fontColor,\n                lineType)\n\ndef read_first_frame(video_path):\n    cap = cv2.VideoCapture(video_path)\n    \n    # Check if the video opened successfully\n    if not cap.isOpened():\n        print(\"Error: Unable to open video.\")\n        return None\n    \n    # Read the first frame\n    ret, frame = cap.read()\n    \n    # Release the video capture object\n    cap.release()\n    \n    # Check if the frame was read successfully\n    if not ret:\n        print(\"Error: Unable to read first frame.\")\n        return None\n    \n    return frame\n\n\nvideo_path = \"/kaggle/working/output2.mp4\"\noutput_video_path = \"labeled_video4.mp4\"\nframes_per_set = 10\n\nfirst_frame = read_first_frame(video_path)\n\nif first_frame is not None:\n    \n    x1,y1,w,h=detect_black_border(first_frame)\n\n\n\ncap = cv2.VideoCapture(video_path)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = int(cap.get(cv2.CAP_PROP_FPS))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n\n\nframes_buffer = []\ntemp=[]\nwhile True:\n    \n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    frames_buffer.append(frame)\n    img=img_preprocess(frame,x1,y1,w,h)\n    temp.append(img)\n\n    if len(frames_buffer) == frames_per_set:\n        prediction = predict_smoke(temp)\n        label = \"Smoke detected\" if prediction == 1 else \"No Smoke\"\n        for frame in frames_buffer:\n            label_frame(frame, label)\n            out.write(frame)\n        frames_buffer = []\n        temp=[]\n\ncap.release()\nout.release()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:21:50.166910Z","iopub.execute_input":"2024-04-13T13:21:50.167399Z","iopub.status.idle":"2024-04-13T13:21:54.756299Z","shell.execute_reply.started":"2024-04-13T13:21:50.167362Z","shell.execute_reply":"2024-04-13T13:21:54.754789Z"},"trusted":true},"execution_count":42,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[42], line 98\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m frames_buffer:\n\u001b[1;32m     97\u001b[0m     label_frame(frame, label)\n\u001b[0;32m---> 98\u001b[0m     \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m frames_buffer \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    100\u001b[0m temp\u001b[38;5;241m=\u001b[39m[]\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}